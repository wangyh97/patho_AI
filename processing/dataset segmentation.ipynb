{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea2dd0a-a295-444a-b8db-8de68357acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import *\n",
    "import os\n",
    "import itertools\n",
    "import gc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e3775-bc7d-4779-b6b2-99eecdd3d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets(object):\n",
    "    seed(10)\n",
    "    def __init__(self,scale:int):\n",
    "\n",
    "        \n",
    "        df = pd.read_csv('../config/patch_info.csv')\n",
    "        df = df[df[f'{scale}x'] != 0]   # remove those patients with no extracted patches under the very scale\n",
    "        self.df = df\n",
    "        self.scale = scale\n",
    "\n",
    "        self.H_count = df['TMB_H/L'].value_counts()['H']\n",
    "        self.L_count = df['TMB_H/L'].value_counts()['L']\n",
    "        # self.H_patches_count = df[df['TMB_H/L']=='H'][f'{scale}x'].sum()\n",
    "        # self.L_patches_count = df[df['TMB_H/L']=='L'][f'{scale}x'].sum()\n",
    "        # self.H_list = df[df['TMB_H/L']=='H']['dir_uuid']\n",
    "        # self.L_list = df[df['TMB_H/L']=='L']['dir_uuid']\n",
    "        \n",
    "    \n",
    "                \n",
    "\n",
    "    \n",
    "    def __segmentation(self):\n",
    "        '''\n",
    "        train:val:test = 0.64:0.16:0.2\n",
    "        percentage of H / L should be balanced among three sets :set shuffle = True\n",
    "        return:\n",
    "            pd.dataframe, patient level segmentation, containing infos of uuid,TMB level,patch_count of each patient under specific scale\n",
    "        '''\n",
    "        df = self.df\n",
    "        uuids = df['dir_uuid']\n",
    "        labels = df['TMB_H/L']\n",
    "\n",
    "        x_tv,x_test,y_tv,y_test = train_test_split(uuids,labels,train_size=0.8,random_state=10,shuffle=True,stratify=labels)\n",
    "        \n",
    "        x_train,x_val,y_train,y_val = train_test_split(x_tv,y_tv,train_size=0.8,random_state=10,shuffle=True,stratify=y_tv)\n",
    "\n",
    "        df_train = df[df['dir_uuid'].isin(x_train)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        df_val = df[df['dir_uuid'].isin(x_val)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        df_test = df[df['dir_uuid'].isin(x_test)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        \n",
    "        def add_img_list(dataframe):\n",
    "            dataframe['path'] = '/mnt/wangyh/CN_patches/' + dataframe['TMB_H/L'] + '/' + dataframe['dir_uuid'] + f'/{self.scale}X/T*'\n",
    "            dataframe['img_list'] = dataframe['path'].apply(glob.glob)\n",
    "            return dataframe\n",
    "    \n",
    "        for i in [df_train,df_val,df_test]:\n",
    "            i = add_img_list(i)\n",
    "   \n",
    "        return df_train,df_val,df_test\n",
    "    def _simclr_segmentation(self):\n",
    "        '''\n",
    "        train + val:train = 0.8:0.2\n",
    "        train + val will be shuffled & resegmented in simclr processing\n",
    "        return:\n",
    "            pd.dataframe, patient level segmentation, containing infos of uuid,TMB level,patch_count of each patient under specific scale\n",
    "        '''\n",
    "        uuids = self.df['dir_uuid']\n",
    "        labels = self.df['TMB_H/L']\n",
    "        \n",
    "        x_tv,x_test,y_tv,y_test = train_test_split(uuids,labels,train_size=0.9,random_state=10,shuffle=True,stratify=labels)\n",
    "\n",
    "        df_tv = self.df[self.df['dir_uuid'].isin(x_tv)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        \n",
    "        def add_img_list(dataframe):\n",
    "            dataframe['path'] = '/mnt/wangyh/CN_patches/' + dataframe['TMB_H/L'] + '/' + dataframe['dir_uuid'] + f'/{self.scale}X/T*'\n",
    "            dataframe['img_list'] = dataframe['path'].apply(glob.glob)\n",
    "            return dataframe\n",
    "    \n",
    "        df_tv = add_img_list(df_tv)\n",
    "   \n",
    "        return df_tv\n",
    "        # num_val_H = int(self.H_count * 0.16)\n",
    "        # num_val_L = int(self.L_count * 0.16)\n",
    "        # num_test_H = int(self.H_count *0.2)\n",
    "        # num_test_L = int(self.L_count * 0.2)\n",
    "        # num_train_H = self.H_count - num_val_H - num_test_H\n",
    "        # num_train_L = self.L_count - num_val_L - num_test_L\n",
    "\n",
    "        # target_patches_val_H = int(self.H_patches_count *0.16)\n",
    "        # target_patches_val_L = int(self.L_patches_count *0.16)\n",
    "        # target_patches_test_H = int(self.H_patches_count *0.2)\n",
    "        # target_patches_test_L = int(self.L_patches_count *0.2)\n",
    "        # target_patches_train_H = self.H_patches_count - target_patches_val_H - target_patches_test_H\n",
    "        # target_patches_train_L = self.L_patches_count - target_patches_val_L - target_patches_test_L\n",
    "\n",
    "\n",
    "        \n",
    "        # dif_test_H = []\n",
    "        # list_test_H = []\n",
    "        # dif_test_L = []\n",
    "        # list_test_L = []\n",
    "        # for cb_test_H in itertools.combinations(self.H_list,num_test_H):\n",
    "        #     patch_count = df[df['dir_uuid'].isin(cb_test_H)][f'{self.scale}x'].sum()\n",
    "        #     print(patch_count)\n",
    "        #     dif_test_H.append(patch_count - target_patches_test_H)\n",
    "        #     list_test_H.append(cb_test_H)\n",
    "        # print(self.H_count,num_test_H)\n",
    "        \n",
    "        # result = pd.DataFrame({\n",
    "        #         'dif':dif_test_H,\n",
    "        #         'combinations':list_test_H\n",
    "        # }\n",
    "        # )\n",
    "        # print(result)\n",
    "\n",
    "\n",
    "        # shuffle(self.H_list)\n",
    "        # shuffle(self.L_list)\n",
    "        \n",
    "        # train_list = self.H_list[:int(self.H_len*0.64)] + self.L_list[:int(self.L_len*0.64)]\n",
    "        # val_list = self.H_list[int(self.H_len*0.64):int(self.H_len*0.8)] + self.L_list[int(self.L_len*0.64):int(self.L_len*0.8)]\n",
    "        # test_list = self.H_list[int(self.H_len*0.8):]+self.L_list[int(self.L_len*0.8):]\n",
    "        # shuffle(train_list)\n",
    "        # shuffle(val_list)\n",
    "        # shuffle(test_list)\n",
    "        # new_img_list = train_list+val_list+test_list\n",
    "        # return train_list,val_list,test_list,new_img_list\n",
    "\n",
    "\n",
    "    # def __combination_distance(self,df,num_patient,TMB_level,target_patches,it = True):\n",
    "    #     '''\n",
    "    #     notice:\n",
    "    #         violate principle of randomization\n",
    "    #     args:\n",
    "    #         df:之前生成的一个Dataframe, 存储有病人的uuid及不同方法倍数下经过extract——patches 及CN操作后得到的patch数量\n",
    "    #         num_patient:希望得到的一个组合的病人数量, 比如5x病人 L:H = 300:64, testing_set病人中H数量就应该为int(64*0.2)\n",
    "    #         TMB_level: H / L, 分别取H / L的病人然后拼成一个集\n",
    "    #         target_patches: 希望得到的这个组合的patch总数, 比如5x病人共有27000张5x图片, 希望testing set中标记为H的patch总数就是27000*(64/364)*0.2\n",
    "    #         it: 用于对比两种方法的开关\n",
    "    #     '''\n",
    "\n",
    "    #     #分别取H / L两组病人,self.H/L_list为该分辨率下patch数不为0的TMB leve为H/L的病人uuid列表\n",
    "    #     if TMB_level == 'H':\n",
    "    #         ls = self.H_list  \n",
    "    #     else:\n",
    "    #         ls = self.L_list\n",
    "    #     dif = 1000000  #int，设置dif的基准值，后续在for循环中更新。表示该病人组合的得到的patch数与目标patch数的差距\n",
    "    #     combinations = ()  #tuple，最优病人组合\n",
    "        \n",
    "    #     #只用在第二种方法的一些参数\n",
    "    #     i = 0 \n",
    "    #     counts = []\n",
    "    #     ls_comb = []\n",
    "\n",
    "\n",
    "    #     for cb_H in itertools.combinations(ls,num_patient):   \n",
    "    #         '''\n",
    "    #         cb_H: combinations of patients H\n",
    "    #         ls:见line 108\n",
    "    #         num_patient:见line 101\n",
    "    #         itertools.combinations(ls,num_patient):\n",
    "    #             generator, 在ls里取num_patient的所有组合,returns a tuple\n",
    "    #             eg:ls = [0,1,2,3],num_patient = 2,则共有C4,2 = 6种取法:(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\n",
    "    #         '''\n",
    "    #         patch_count = df[df['dir_uuid'].isin(cb_H)][f'{self.scale}x'].sum() #patch count为对这个病人组合的patch数求和得到的patch总数\n",
    "\n",
    "    #         distance = abs(patch_count-target_patches)  #distance表示patch count与target_patches的绝对距离，距离越短的认为越优\n",
    "    #         #方法1：一个一个对比，取绝对距离最小的那个\n",
    "    #         if it:\n",
    "    #             if distance < dif:\n",
    "    #                 dif = distance\n",
    "    #                 combinations = cb_H\n",
    "    #         #方法2：每个批次取200个，先将200个的patch_count（这里用count）和病人组合（这里用ls_comb）存进列表，然后转换为numpy，取批次最小值，然后每个批次对比\n",
    "    #         else:\n",
    "    #             i+=1\n",
    "    #             counts.append(patch_count)\n",
    "    #             ls_comb.append(cb_H)\n",
    "    #             counts_np = np.absolute(np.array(counts)-target_patches)\n",
    "    #             distance = np.min(counts_np)\n",
    "    #             index = np.argmin(counts_np)\n",
    "    #             comb = ls_comb[index]\n",
    "    #             if i%200 ==0:\n",
    "    #                 if distance < dif:\n",
    "    #                     dif = distance\n",
    "    #                     combinations = comb\n",
    "    #                 counts = []\n",
    "    #                 ls_comb = []\n",
    "\n",
    "    #     return dif,combinations\n",
    "\n",
    "    # def get(self):\n",
    "    #     return self.__segmentation()\n",
    "\n",
    "    '''\n",
    "    get uuids & labels of different datasets\n",
    "    '''\n",
    "    def get_train(self):\n",
    "        return self.__segmentation()[0]\n",
    "    \n",
    "    def get_val(self):\n",
    "        return self.__segmentation()[1]\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.__segmentation()[2]\n",
    "    \n",
    "    def get_tv(self):\n",
    "        return self._simclr_segmentation()\n",
    "    \n",
    "    def get_csv(self,dir='../config/data_segmentation_csv'):\n",
    "        '''\n",
    "        return:\n",
    "            npy file, saving dict of dataframes of [train,val,test] dataset\n",
    "        '''\n",
    "        #notice: adjust the path for data saving\n",
    "        # new_img_list = self.__segmentation()[3]\n",
    "        # ground_truth = {path:path.split('/')[4] for path in new_img_list}\n",
    "        \n",
    "        # df = pd.DataFrame(list(ground_truth.items()))\n",
    "        # #save ground truth as csv\n",
    "        \n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        # path_csv = os.path.join(dir,f'{self.scale}X_ground_truth.csv')\n",
    "#         df.to_csv(path_csv)\n",
    "        \n",
    "        #save grouping as pickle\n",
    "        grouping = {\n",
    "            'train_list':self.get_train(),\n",
    "            'val_list': self.get_val(),\n",
    "            'test_list':self.get_test()\n",
    "        }\n",
    "        \n",
    "        path_grouping = os.path.join(dir,f'{self.scale}X_grouping.npy')\n",
    "        np.save(path_grouping,grouping)\n",
    "    \n",
    "    def get_tv_csv(self,dir='../config/data_segmentation_csv'):\n",
    "        '''\n",
    "        return:\n",
    "            npy file, saving dict of dataframes of [tv] dataset\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "\n",
    "        grouping = {\n",
    "            'tv_list':self.get_tv()\n",
    "        }\n",
    "        \n",
    "        path_grouping = os.path.join(dir,f'{self.scale}X_tv_grouping.npy')\n",
    "        np.save(path_grouping,grouping)\n",
    "    \n",
    "    def get_full_csv(self,dir='../config/data_segmentation_csv'):\n",
    "        '''\n",
    "        return:\n",
    "            npy file, saving dict of dataframes of [full] dataset\n",
    "        '''\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "            \n",
    "        def add_img_list(dataframe):\n",
    "            dataframe['path'] = '/mnt/wangyh/CN_patches/' + dataframe['TMB_H/L'] + '/' + dataframe['dir_uuid'] + f'/{self.scale}X/T*'\n",
    "            dataframe['img_list'] = dataframe['path'].apply(glob.glob)\n",
    "            return dataframe\n",
    "        \n",
    "        full_df = add_img_list(self.df)\n",
    "        grouping = {\n",
    "            'full_list':full_df\n",
    "        }\n",
    "        \n",
    "        path_grouping = os.path.join(dir,f'{self.scale}X_full.npy')\n",
    "        np.save(path_grouping,grouping)\n",
    "    \n",
    "    def __getattribute__(self, name: str) :\n",
    "        return object.__getattribute__(self,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47d329e-ebe4-440b-bce6-8cb0fe70040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5x = datasets(5)\n",
    "data_10x = datasets(10)\n",
    "data_20x = datasets(20)\n",
    "data_40x = datasets(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_5x.get_full_csv()\n",
    "# data_10x.get_full_csv()\n",
    "# data_20x.get_full_csv()\n",
    "# data_40x.get_full_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../config/data_segmentation_csv/40X_full.npy',allow_pickle=True).item()\n",
    "data['full_list']['img_list'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8db5c",
   "metadata": {},
   "source": [
    "一个一个对比花费23.9s（num_patients = 3） 6m47s（num_patients = 4）\n",
    "\n",
    "按batch对比与一个一个对比的时间花费对比: \n",
    "\n",
    "|num_patient|one by one|i=10|i=50|i=100|i=200|i=1000|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|3|23.9|26.3|25.5|24.2|25.7|27.1|\n",
    "|4|6m47||6min54|6m27s|6m13s|\n",
    "\n",
    "1.num_patient = 3:i =10 使用26.3s，i=50使用25.5，i=100 使用24.2s，i=200 使用 25.7s，i=500使用24.9s，i=1000使用27.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81009a07-6728-44a9-b3bf-2e30c978584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21160409556313994 0.2140468227424749 0.21172638436482086 0.2129032258064516\n"
     ]
    }
   ],
   "source": [
    "print(data_5x.H_count / data_5x.L_count,data_10x.H_count/data_10x.L_count,data_20x.H_count/data_20x.L_count,data_40x.H_count/data_40x.L_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e85eecbf-bbaf-45dd-ba45-88c97f10cc8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "('img_list', [])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '('img_list', [])' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m val_df \u001b[38;5;241m=\u001b[39m grouping[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_list\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_list\u001b[39m\u001b[38;5;124m'\u001b[39m],[])\n\u001b[0;32m----> 5\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mval_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/core/indexes/base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3628\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3631\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/patho_AI/lib/python3.8/site-packages/pandas/core/indexes/base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5635\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5636\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: ('img_list', [])"
     ]
    }
   ],
   "source": [
    "grouping = np.load(f'../config/data_segmentation_csv/{10}X_grouping.npy',allow_pickle=True).item()\n",
    "train_df = grouping['train_list']\n",
    "val_df = grouping['val_list']\n",
    "train = sum(train_df['img_list'],[])\n",
    "val = sum(val_df['img_list',[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f348fcc3-abce-4663-badf-71f787108405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "gpus = '1,2,3'\n",
    "len(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374e0c9-14a3-4711-9017-c8c278e5add9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patho_AI",
   "language": "python",
   "name": "patho_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
