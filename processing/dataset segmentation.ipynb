{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea2dd0a-a295-444a-b8db-8de68357acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import *\n",
    "import os\n",
    "import itertools\n",
    "import gc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c9e3775-bc7d-4779-b6b2-99eecdd3d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets(object):\n",
    "    seed(10)\n",
    "    def __init__(self,scale:int):\n",
    "\n",
    "        \n",
    "        df = pd.read_csv('../config/patch_info.csv')\n",
    "        df = df[df[f'{scale}x'] != 0]   # remove those patients with no extracted patches under the very scale\n",
    "        self.df = df\n",
    "        self.scale = scale\n",
    "\n",
    "        # self.H_count = df['TMB_H/L'].value_counts()['H']\n",
    "        # self.L_count = df['TMB_H/L'].value_counts()['L']\n",
    "        # self.H_patches_count = df[df['TMB_H/L']=='H'][f'{scale}x'].sum()\n",
    "        # self.L_patches_count = df[df['TMB_H/L']=='L'][f'{scale}x'].sum()\n",
    "        # self.H_list = df[df['TMB_H/L']=='H']['dir_uuid']\n",
    "        # self.L_list = df[df['TMB_H/L']=='L']['dir_uuid']\n",
    "        \n",
    "    \n",
    "                \n",
    "    \n",
    "    def __segmentation(self):\n",
    "        '''\n",
    "        train:val:train = 0.64:0.16:0.2\n",
    "        percentage of H / L should be balanced among three sets :set shuffle = True\n",
    "        '''\n",
    "        df = self.df\n",
    "        uuids = df['dir_uuid']\n",
    "        labels = df['TMB_H/L']\n",
    "\n",
    "        x_tv,x_test,y_tv,y_test = train_test_split(uuids,labels,train_size=0.8,random_state=10,shuffle=True,stratify=labels)\n",
    "        \n",
    "        x_train,x_val,y_train,y_val = train_test_split(x_tv,y_tv,train_size=0.8,random_state=10,shuffle=True,stratify=y_tv)\n",
    "\n",
    "        df_train = df[df['dir_uuid'].isin(x_train)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        df_val = df[df['dir_uuid'].isin(x_val)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        df_test = df[df['dir_uuid'].isin(x_test)][['dir_uuid','TMB_H/L',f'{self.scale}x']]\n",
    "        \n",
    "        return df_train,df_val,df_test\n",
    "        # num_val_H = int(self.H_count * 0.16)\n",
    "        # num_val_L = int(self.L_count * 0.16)\n",
    "        # num_test_H = int(self.H_count *0.2)\n",
    "        # num_test_L = int(self.L_count * 0.2)\n",
    "        # num_train_H = self.H_count - num_val_H - num_test_H\n",
    "        # num_train_L = self.L_count - num_val_L - num_test_L\n",
    "\n",
    "        # target_patches_val_H = int(self.H_patches_count *0.16)\n",
    "        # target_patches_val_L = int(self.L_patches_count *0.16)\n",
    "        # target_patches_test_H = int(self.H_patches_count *0.2)\n",
    "        # target_patches_test_L = int(self.L_patches_count *0.2)\n",
    "        # target_patches_train_H = self.H_patches_count - target_patches_val_H - target_patches_test_H\n",
    "        # target_patches_train_L = self.L_patches_count - target_patches_val_L - target_patches_test_L\n",
    "\n",
    "\n",
    "        \n",
    "        # dif_test_H = []\n",
    "        # list_test_H = []\n",
    "        # dif_test_L = []\n",
    "        # list_test_L = []\n",
    "        # for cb_test_H in itertools.combinations(self.H_list,num_test_H):\n",
    "        #     patch_count = df[df['dir_uuid'].isin(cb_test_H)][f'{self.scale}x'].sum()\n",
    "        #     print(patch_count)\n",
    "        #     dif_test_H.append(patch_count - target_patches_test_H)\n",
    "        #     list_test_H.append(cb_test_H)\n",
    "        # print(self.H_count,num_test_H)\n",
    "        \n",
    "        # result = pd.DataFrame({\n",
    "        #         'dif':dif_test_H,\n",
    "        #         'combinations':list_test_H\n",
    "        # }\n",
    "        # )\n",
    "        # print(result)\n",
    "\n",
    "\n",
    "        # shuffle(self.H_list)\n",
    "        # shuffle(self.L_list)\n",
    "        \n",
    "        # train_list = self.H_list[:int(self.H_len*0.64)] + self.L_list[:int(self.L_len*0.64)]\n",
    "        # val_list = self.H_list[int(self.H_len*0.64):int(self.H_len*0.8)] + self.L_list[int(self.L_len*0.64):int(self.L_len*0.8)]\n",
    "        # test_list = self.H_list[int(self.H_len*0.8):]+self.L_list[int(self.L_len*0.8):]\n",
    "        # shuffle(train_list)\n",
    "        # shuffle(val_list)\n",
    "        # shuffle(test_list)\n",
    "        # new_img_list = train_list+val_list+test_list\n",
    "        # return train_list,val_list,test_list,new_img_list\n",
    "\n",
    "\n",
    "    # def __combination_distance(self,df,num_patient,TMB_level,target_patches,it = True):\n",
    "    #     '''\n",
    "    #     notice:\n",
    "    #         violate principle of randomization\n",
    "    #     args:\n",
    "    #         df:之前生成的一个Dataframe, 存储有病人的uuid及不同方法倍数下经过extract——patches 及CN操作后得到的patch数量\n",
    "    #         num_patient:希望得到的一个组合的病人数量, 比如5x病人 L:H = 300:64, testing_set病人中H数量就应该为int(64*0.2)\n",
    "    #         TMB_level: H / L, 分别取H / L的病人然后拼成一个集\n",
    "    #         target_patches: 希望得到的这个组合的patch总数, 比如5x病人共有27000张5x图片, 希望testing set中标记为H的patch总数就是27000*(64/364)*0.2\n",
    "    #         it: 用于对比两种方法的开关\n",
    "    #     '''\n",
    "\n",
    "    #     #分别取H / L两组病人,self.H/L_list为该分辨率下patch数不为0的TMB leve为H/L的病人uuid列表\n",
    "    #     if TMB_level == 'H':\n",
    "    #         ls = self.H_list  \n",
    "    #     else:\n",
    "    #         ls = self.L_list\n",
    "    #     dif = 1000000  #int，设置dif的基准值，后续在for循环中更新。表示该病人组合的得到的patch数与目标patch数的差距\n",
    "    #     combinations = ()  #tuple，最优病人组合\n",
    "        \n",
    "    #     #只用在第二种方法的一些参数\n",
    "    #     i = 0 \n",
    "    #     counts = []\n",
    "    #     ls_comb = []\n",
    "\n",
    "\n",
    "    #     for cb_H in itertools.combinations(ls,num_patient):   \n",
    "    #         '''\n",
    "    #         cb_H: combinations of patients H\n",
    "    #         ls:见line 108\n",
    "    #         num_patient:见line 101\n",
    "    #         itertools.combinations(ls,num_patient):\n",
    "    #             generator, 在ls里取num_patient的所有组合,returns a tuple\n",
    "    #             eg:ls = [0,1,2,3],num_patient = 2,则共有C4,2 = 6种取法:(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\n",
    "    #         '''\n",
    "    #         patch_count = df[df['dir_uuid'].isin(cb_H)][f'{self.scale}x'].sum() #patch count为对这个病人组合的patch数求和得到的patch总数\n",
    "\n",
    "    #         distance = abs(patch_count-target_patches)  #distance表示patch count与target_patches的绝对距离，距离越短的认为越优\n",
    "    #         #方法1：一个一个对比，取绝对距离最小的那个\n",
    "    #         if it:\n",
    "    #             if distance < dif:\n",
    "    #                 dif = distance\n",
    "    #                 combinations = cb_H\n",
    "    #         #方法2：每个批次取200个，先将200个的patch_count（这里用count）和病人组合（这里用ls_comb）存进列表，然后转换为numpy，取批次最小值，然后每个批次对比\n",
    "    #         else:\n",
    "    #             i+=1\n",
    "    #             counts.append(patch_count)\n",
    "    #             ls_comb.append(cb_H)\n",
    "    #             counts_np = np.absolute(np.array(counts)-target_patches)\n",
    "    #             distance = np.min(counts_np)\n",
    "    #             index = np.argmin(counts_np)\n",
    "    #             comb = ls_comb[index]\n",
    "    #             if i%200 ==0:\n",
    "    #                 if distance < dif:\n",
    "    #                     dif = distance\n",
    "    #                     combinations = comb\n",
    "    #                 counts = []\n",
    "    #                 ls_comb = []\n",
    "\n",
    "    #     return dif,combinations\n",
    "\n",
    "    # def get(self):\n",
    "    #     return self.__segmentation()\n",
    "\n",
    "    '''\n",
    "    get uuids & labels of different datasets\n",
    "    '''\n",
    "    def get_train(self):\n",
    "        return self.__segmentation()[0]\n",
    "    \n",
    "    def get_val(self):\n",
    "        return self.__segmentation()[1]\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.__segmentation()[2]\n",
    "    \n",
    "    def get_csv(self,dir='../config/data_segmentation_csv'):\n",
    "        #notice: adjust the path for data saving\n",
    "        # new_img_list = self.__segmentation()[3]\n",
    "        # ground_truth = {path:path.split('/')[4] for path in new_img_list}\n",
    "        \n",
    "        # df = pd.DataFrame(list(ground_truth.items()))\n",
    "        # #save ground truth as csv\n",
    "        \n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        # path_csv = os.path.join(dir,f'{self.scale}X_ground_truth.csv')\n",
    "#         df.to_csv(path_csv)\n",
    "        \n",
    "        #save grouping as pickle\n",
    "        grouping = {\n",
    "            'train_list':self.get_train(),\n",
    "            'val_list': self.get_val(),\n",
    "            'test_list':self.get_test()\n",
    "        }\n",
    "        \n",
    "        path_grouping = os.path.join(dir,f'{self.scale}X_grouping.npy')\n",
    "        np.save(path_grouping,grouping)\n",
    "\n",
    "    def __getattribute__(self, name: str) :\n",
    "        return object.__getattribute__(self,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a47d329e-ebe4-440b-bce6-8cb0fe70040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5x = datasets(5)\n",
    "data_10x = datasets(10)\n",
    "data_20x = datasets(20)\n",
    "data_40x = datasets(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "116802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5x.get_csv()\n",
    "data_10x.get_csv()\n",
    "data_20x.get_csv()\n",
    "data_40x.get_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c59fdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir_uuid</th>\n",
       "      <th>TMB_H/L</th>\n",
       "      <th>5x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bff29d20-3a8f-4a5d-a2de-0e142390551d</td>\n",
       "      <td>L</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edcf4ae6-c985-40ad-aff4-a0ce31b46aeb</td>\n",
       "      <td>L</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e20c2176-2b57-4cc9-a68a-eb6933bf60b1</td>\n",
       "      <td>L</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70e383bf-91cb-4a65-b12a-950f365c4d62</td>\n",
       "      <td>L</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>790d96d7-1191-4cf7-9c78-3e879723afd8</td>\n",
       "      <td>L</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>278e745d-72e3-4538-8b07-0b9157e605d3</td>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>613f9ee2-07d2-4ec7-98b5-335e3eb164cd</td>\n",
       "      <td>H</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>05e99737-1738-432d-bf0e-7ca0bbb11c31</td>\n",
       "      <td>L</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>f6f1c724-96f3-45ac-ab48-446fca061d23</td>\n",
       "      <td>L</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>16c68e40-fc90-495a-b856-fe1f11f82143</td>\n",
       "      <td>L</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 dir_uuid TMB_H/L   5x\n",
       "1    bff29d20-3a8f-4a5d-a2de-0e142390551d       L  265\n",
       "2    edcf4ae6-c985-40ad-aff4-a0ce31b46aeb       L  229\n",
       "3    e20c2176-2b57-4cc9-a68a-eb6933bf60b1       L    3\n",
       "5    70e383bf-91cb-4a65-b12a-950f365c4d62       L  135\n",
       "6    790d96d7-1191-4cf7-9c78-3e879723afd8       L  305\n",
       "..                                    ...     ...  ...\n",
       "371  278e745d-72e3-4538-8b07-0b9157e605d3       H    6\n",
       "372  613f9ee2-07d2-4ec7-98b5-335e3eb164cd       H   22\n",
       "374  05e99737-1738-432d-bf0e-7ca0bbb11c31       L  180\n",
       "375  f6f1c724-96f3-45ac-ab48-446fca061d23       L   92\n",
       "377  16c68e40-fc90-495a-b856-fe1f11f82143       L   47\n",
       "\n",
       "[227 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('../config/data_segmentation_csv/5X_grouping.npy',allow_pickle=True).item()\n",
    "data['train_list']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46c8db5c",
   "metadata": {},
   "source": [
    "一个一个对比花费23.9s（num_patients = 3） 6m47s（num_patients = 4）\n",
    "\n",
    "按batch对比与一个一个对比的时间花费对比: \n",
    "\n",
    "|num_patient|one by one|i=10|i=50|i=100|i=200|i=1000|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|3|23.9|26.3|25.5|24.2|25.7|27.1|\n",
    "|4|6m47||6min54|6m27s|6m13s|\n",
    "\n",
    "1.num_patient = 3:i =10 使用26.3s，i=50使用25.5，i=100 使用24.2s，i=200 使用 25.7s，i=500使用24.9s，i=1000使用27.1s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
