import random
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.autograd import Variable
import torchvision.transforms.functional as VF
from torchvision import transforms

import sys, argparse, os, copy, itertools, glob, datetime
import pandas as pd
import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support
from sklearn.datasets import load_svmlight_file
from collections import OrderedDict
import time

'''
####################################
2023-5-11 train with resnet18 extracted features:
    read from new feature files, rewrite relative path due to difference between sh.file & py.file in path
####################################
2023-5-14 updated:
    a. 重构了train_ptres/train_tcga，加入--record参数，默认为False模式（参数寻找模式），若为True则为模型获得模式，并进行：
        i. 激活tensorboard，将文件储存在：f'run/lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}'文件夹，每次训练会获得一个events
        ii. 记录模型，模型保存在：f'weights/lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}'文件夹，获得：
            1) Best_score.pth
            2) Best_auc.pth
            3) Best_avg_auc.pth
    *** 改动在train_tcga同步
2023-5-15 updated:
    writer.add_scalar('val_loss',test_loss_bag,epoch)
2023-5-16 update:
    change the naming strategy of saved pth files
'''
# load data

# =====>>>> current working dir is where the bash file located, not the executed py file <<<<==============

features = np.load('../../../data/pretrained_resnet18/10X_full_slide_features_PtRes18.npy', allow_pickle=True).item()
# train = np.load('../../config/data_segmentation_csv/10X_tv_grouping.npy',allow_pickle=True).item()
full = np.load('../../../config/data_segmentation_csv/10X_full.npy', allow_pickle=True).item()
# train_index = list(train['tv_list'].index)
full_index = list(full['full_list'].index)
# test_index = list(set(full_index)-set(train_index))
grouping = np.load('../../../config/data_segmentation_csv/10X_grouping.npy', allow_pickle=True).item()
train_index = grouping['train_list'].index
test_index = grouping['val_list'].index

index_dict = {ind: i for i, ind in enumerate(
    full_index)}  # feature dict -- index{i} is generated by iloc, meaning i is continuous index except the original uncontinuous ones, need transform using index read from grouping infos
label_dict = {'L': 0, 'H': 1}


def set_seed(seed=10):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    print('seed set')


#     torch.backends.cudnn.benchmark = False
#     torch.backends.cudnn.deterministic = True


def get_bag_feats(index, args):
    #     if args.dataset == 'TCGA-lung-default':
    #         feats_csv_path = 'datasets/tcga-dataset/tcga_lung_data_feats/' + csv_file_df.iloc[0].split('/')[1] + '.csv'
    #     else:
    #         feats_csv_path = csv_file_df.iloc[0]
    #     df = pd.read_csv(feats_csv_path)
    #     feats = shuffle(df).reset_index(drop=True)
    #     feats = feats.to_numpy()
    #     label = np.zeros(args.num_classes)
    #     if args.num_classes==1:
    #         label[0] = csv_file_df.iloc[1]
    #     else:
    #         if int(csv_file_df.iloc[1])<=(len(label)-1):
    #             label[int(csv_file_df.iloc[1])] = 1
    #     labels = [feature[f'index{index_dict[i]}'][1] for i in indexes]  #list of labels, in form of ['H','L','L'...]
    #     feats = [feature[f'index{index_dict[i]}'][2] for i in indexes]

    global features
    global index_dict

    feats_og = pd.DataFrame(features[f'index{index_dict[index]}'][2])
    feats = shuffle(feats_og).reset_index(drop=True).to_numpy()
    label_og = label_dict[features[f'index{index_dict[index]}'][1]]  # transformed label in form of int,[0,1]

    label = np.zeros(args.num_classes)
    if args.num_classes == 1:
        label[0] = label_og
    else:
        if int(label_og) <= (len(label) - 1):
            label[int(label_og)] = 1
    return label, feats


def train(train_index, milnet, criterion, optimizer, args):
    milnet.train()
    train_index = shuffle(train_index)
    total_loss = 0
    bc = 0
    Tensor = torch.cuda.FloatTensor
    #     Tensor = torch.FloatTensor
    for i in range(len(train_index)):
        optimizer.zero_grad()
        label, feats = get_bag_feats(train_index[i], args)
        feats = dropout_patches(feats, args.dropout_patch)
        bag_label = Variable(Tensor([label]))
        bag_feats = Variable(Tensor([feats]))
        bag_feats = bag_feats.view(-1, args.feats_size)
        ins_prediction, bag_prediction, _, _ = milnet(
            bag_feats)  # 使用多卡会出现多组bag prediciton，使计算bag loss时criterion时bag prediction / bag labelshape不匹配 -- 单卡多线程
        max_prediction, _ = torch.max(ins_prediction, 0)
        bag_loss = criterion(bag_prediction.view(1, -1), bag_label.view(1, -1))
        max_loss = criterion(max_prediction.view(1, -1), bag_label.view(1, -1))
        loss = 0.5 * bag_loss + 0.5 * max_loss
        loss.backward()
        optimizer.step()
        total_loss = total_loss + loss.item()
    #         sys.stdout.write('\r Training bag [%d/%d] bag loss: %.4f' % (i, len(train_index), loss.item()))
    return total_loss / len(train_index)


def dropout_patches(feats, p):
    idx = np.random.choice(np.arange(feats.shape[0]), int(feats.shape[0] * (1 - p)), replace=False)
    sampled_feats = np.take(feats, idx, axis=0)
    pad_idx = np.random.choice(np.arange(sampled_feats.shape[0]), int(feats.shape[0] * p), replace=False)
    pad_feats = np.take(sampled_feats, pad_idx, axis=0)
    sampled_feats = np.concatenate((sampled_feats, pad_feats), axis=0)
    return sampled_feats


def test(test_df, milnet, criterion, optimizer, args):
    milnet.eval()
    total_loss = 0
    test_labels = []
    test_predictions = []
    Tensor = torch.cuda.FloatTensor
    #     Tensor = torch.FloatTensor
    with torch.no_grad():
        for i in range(len(test_df)):
            label, feats = get_bag_feats(test_df[i], args)
            bag_label = Variable(Tensor([label]))
            bag_feats = Variable(Tensor([feats]))
            bag_feats = bag_feats.view(-1, args.feats_size)
            ins_prediction, bag_prediction, _, _ = milnet(bag_feats)
            max_prediction, _ = torch.max(ins_prediction, 0)
            bag_prediction = torch.mean(bag_prediction, dim=0)
            bag_loss = criterion(bag_prediction.view(1, -1), bag_label.view(1, -1))
            max_loss = criterion(max_prediction.view(1, -1), bag_label.view(1, -1))
            loss = 0.5 * bag_loss + 0.5 * max_loss
            total_loss = total_loss + loss.item()
            sys.stdout.write('\r Testing bag [%d/%d] bag loss: %.4f' % (i, len(test_df), loss.item()))
            test_labels.extend([label])
            if args.average:
                test_predictions.extend([(0.5 * torch.sigmoid(max_prediction) + 0.5 * torch.sigmoid(
                    bag_prediction)).squeeze().cpu().numpy()])
            else:
                test_predictions.extend([(0.0 * torch.sigmoid(max_prediction) + 1.0 * torch.sigmoid(
                    bag_prediction)).squeeze().cpu().numpy()])
    test_labels = np.array(test_labels)
    test_predictions = np.array(test_predictions)
    auc_value, _, thresholds_optimal = multi_label_roc(test_labels, test_predictions, args.num_classes, pos_label=1)
    if args.num_classes == 1:
        class_prediction_bag = copy.deepcopy(test_predictions)
        class_prediction_bag[test_predictions >= thresholds_optimal[0]] = 1
        class_prediction_bag[test_predictions < thresholds_optimal[0]] = 0
        test_predictions = class_prediction_bag
        test_labels = np.squeeze(test_labels)
    else:
        for i in range(args.num_classes):
            class_prediction_bag = copy.deepcopy(test_predictions[:, i])
            class_prediction_bag[test_predictions[:, i] >= thresholds_optimal[i]] = 1
            class_prediction_bag[test_predictions[:, i] < thresholds_optimal[i]] = 0
            test_predictions[:, i] = class_prediction_bag
    bag_score = 0
    for i in range(0, len(test_df)):
        bag_score = np.array_equal(test_labels[i], test_predictions[i]) + bag_score
    avg_score = bag_score / len(test_df)

    return total_loss / len(test_df), avg_score, auc_value, thresholds_optimal


def multi_label_roc(labels, predictions, num_classes, pos_label=1):
    fprs = []
    tprs = []
    thresholds = []
    thresholds_optimal = []
    aucs = []
    if len(predictions.shape) == 1:
        predictions = predictions[:, None]
    for c in range(0, num_classes):
        label = labels[:, c]
        prediction = predictions[:, c]
        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=1)
        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)
        c_auc = roc_auc_score(label, prediction)
        aucs.append(c_auc)
        thresholds.append(threshold)
        thresholds_optimal.append(threshold_optimal)
    return aucs, thresholds, thresholds_optimal

def recording(record,net,saving_path,current_score,best_score,aucs,best_auc,best_avg_auc,best_loss,test_loss_bag,epoch,thresholds_optimal):
    # return:
    def model_recorder(record,net,metrics,saving_path,epoch):
        # record should be ['none','best','all']
        # if record == none, weight will not be saved, else, weights will be saved with name generated by recorder every epoch after conditional judgement
        if record == 'none':
            pass
        elif record == 'best':
            save_name = os.path.join(saving_path, f'{metrics}.pth')
        elif record == 'all':
            save_name = os.path.join(saving_path, str(epoch)+f'{metrics}.pth')
        if save_name:
            torch.save(net.state_dict(), save_name)

    # print infos with or w/o saving models
    if current_score >= best_score:
        best_score = current_score
        print('Best_score thresholds ===>>> ' + '|'.join(
            'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        model_recorder(record,net,'best_score',saving_path,epoch)

    if aucs[1] >= best_auc:
        best_auc = aucs[1]
        print('Best_auc thresholds ===>>> ' + '|'.join(
            'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        print(f'Best auc ===>>>{best_auc}')
        model_recorder(record, net,'best_auc', saving_path, epoch)

    if sum(aucs) >= best_avg_auc:
        best_avg_auc = sum(aucs)
        print('Best_avg_auc thresholds ===>>> ' + '|'.join(
            'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        print('best avg_auc ===>>>' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
        model_recorder(record, net, 'best_auc', saving_path, epoch)

    if best_loss == 0:
        best_loss = test_loss_bag
        if test_loss_bag < best_loss:
            best_loss = test_loss_bag
            best_loss_score = current_score
            best_loss_auc = aucs[1]
            best_loss_avg_auc = sum(aucs)
            best_loss_epoch = epoch
    return best_score,best_auc,best_avg_auc,best_loss_epoch,best_loss_score,best_loss_auc,best_loss_avg_auc

def train_epoch(train_index,test_index,milnet,criterion,optimizer,args,scheduler):
    best_score = 0
    best_auc = 0
    best_avg_auc = 0
    best_loss = 0
    best_loss_score = 0
    best_loss_auc = 0
    best_loss_avg_auc = 0
    best_loss_epoch = 0

    for epoch in range(1, args.epochs):
        train_index = shuffle(train_index)
        test_index = shuffle(test_index)

        train_loss_bag = train(train_index, milnet, criterion, optimizer, args)  # iterate all bags
        test_loss_bag, avg_score, aucs, thresholds_optimal = test(test_index, milnet, criterion, optimizer, args)
        print('\r Epoch [%d/%d] train loss: %.4f test loss: %.4f, average score: %.4f, AUC: ' %
              (epoch, args.num_epochs, train_loss_bag, test_loss_bag, avg_score) + '|'.join(
            'class-{}>>{}'.format(*k) for k in enumerate(aucs)))
        scheduler.step()
        current_score = (sum(aucs) + avg_score) / 2  # 均衡考虑auc与accuracy

        if args.record != 'none':
            writer = SummaryWriter(f'run/lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}')  # path saving tensorboard logdir
            save_path = os.path.join('weights', f'lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}')  # path saving models
            os.makedirs(save_path, exist_ok=True)
            writer.add_scalar('train_loss', train_loss_bag, epoch)
            writer.add_scalar('val_loss', test_loss_bag, epoch)
            writer.add_scalar('avg_score', avg_score, epoch)
            writer.add_scalar('class 0 aucs', aucs[0], epoch)
            writer.add_scalar('class_1 aucs', aucs[1], epoch)
        else:
            save_path = ''

        best_score, best_auc, best_avg_auc, best_loss_epoch, best_loss_score, best_loss_auc, best_loss_avg_auc = recording(
            args.record, milnet, save_path, current_score, best_score, aucs, best_auc, best_avg_auc, best_loss, test_loss_bag,
            epoch, thresholds_optimal
        )

        # if current_score >= best_score:
        #     best_score = current_score
        #     print('Best_score thresholds ===>>> ' + '|'.join(
        #         'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        #
        #
        # if aucs[1] >= best_auc:
        #     best_auc = aucs[1]
        #     print('Best_auc thresholds ===>>> ' + '|'.join(
        #         'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        #     print(f'Best auc ===>>>{best_auc}')
        #
        # if sum(aucs) >= best_avg_auc:
        #     best_avg_auc = sum(aucs)
        #     print('Best_avg_auc thresholds ===>>> ' + '|'.join(
        #         'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
        #     print('best avg_auc ===>>>' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
        # if best_loss == 0:
        #     best_loss = test_loss_bag
        #     if test_loss_bag < best_loss:
        #         best_loss = test_loss_bag
        #         best_loss_score = current_score
        #         best_loss_auc = aucs[1]
        #         best_loss_avg_auc = sum(aucs)
        #         best_loss_epoch = epoch
    with open(f'results_{args.run}.txt', 'a+') as f:
        f.write(f'{args.lr},{args.weight_decay},{args.Tmax},{best_score},{best_auc},{best_avg_auc},\
        {best_loss_epoch},{best_loss_score},{best_loss_auc},{best_loss_avg_auc}\n')
def optimal_thresh(fpr, tpr, thresholds, p=0):
    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)
    idx = np.argmin(loss, axis=0)
    return fpr[idx], tpr[idx], thresholds[idx]


def main():
    parser = argparse.ArgumentParser(description='Train DSMIL on 20x patch features learned by SimCLR')
    # to finetune
    parser.add_argument('--lr', default=0.0002, type=float, help='Initial learning rate [0.0002]')
    parser.add_argument('--weight_decay', default=5e-3, type=float, help='Weight decay [5e-3]')
    parser.add_argument('--Tmax', default=50, type=int, help='Tmax used in CosineAnnealingLR,choose from [200,100,50]')
    # fixed args
    parser.add_argument('--num_classes', default=2, type=int, help='Number of output classes [2]')
    parser.add_argument('--feats_size', default=512, type=int, help='Dimension of the feature size [512]')
    parser.add_argument('--num_epochs', default=200, type=int, help='Number of total training epochs [40|200]')
    parser.add_argument('--gpu_index', type=str, help='GPU ID(s) [0]')
    parser.add_argument('--model', default='dsmil', type=str, help='MIL model [dsmil]')
    parser.add_argument('--dropout_patch', default=0, type=float, help='Patch dropout rate [0]')
    parser.add_argument('--dropout_node', default=0, type=float, help='Bag classifier dropout rate [0]')
    parser.add_argument('--non_linearity', default=1, type=float, help='Additional nonlinear operation [0]')
    parser.add_argument('--average', type=bool, default=True,
                        help='Average the score of max-pooling and bag aggregating')
    parser.add_argument('--run', type=str,
                        help='run number for documentation, saving results in file: results{run}.txt')
    parser.add_argument('--record', type=str, default='best',
                        help='activate tensorboard and record, save the best model, result file will be generated anyway,choose from [best,all,none][best]')
    args = parser.parse_args()
    #     gpu_ids = args.gpu_index
    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_index)
    set_seed()
    if args.model == 'dsmil':
        import dsmil as mil
    elif args.model == 'abmil':
        import abmil as mil

    i_classifier = mil.FCLayer(in_size=args.feats_size, out_size=args.num_classes).cuda()
    b_classifier = mil.BClassifier(input_size=args.feats_size, output_class=args.num_classes,
                                   dropout_v=args.dropout_node, nonlinear=args.non_linearity).cuda()
    milnet = mil.MILNet(i_classifier, b_classifier).cuda()

    milnet = torch.nn.DataParallel(milnet)
    milnet = milnet.cuda()

    if args.model == 'dsmil':
        state_dict_weights = torch.load('../init.pth')
        try:
            milnet.load_state_dict(state_dict_weights, strict=False)
        except:
            del state_dict_weights['b_classifier.v.1.weight']
            del state_dict_weights['b_classifier.v.1.bias']
            milnet.load_state_dict(state_dict_weights, strict=False)
    criterion = nn.BCEWithLogitsLoss()

    optimizer = torch.optim.Adam(milnet.parameters(), lr=args.lr, betas=(0.5, 0.9), weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.Tmax, 0.000005)

    # load data
    global train_index
    global test_index

    train_epoch(train_index,test_index,milnet,criterion,optimizer,args,scheduler)

    # best_score = 0
    # best_auc = 0
    # best_avg_auc = 0
    # best_loss = 0
    # best_loss_score = 0
    # best_loss_auc = 0
    # best_loss_avg_auc =0
    # best_loss_epoch = 0
    #
    # if args.record == 'none':
    #     # if not record，run & weights files wouldn't be generated，used in [***nuisance hyperparameter searching***]
    #     for epoch in range(1, args.num_epochs):
    #         train_index = shuffle(train_index)
    #         test_index = shuffle(test_index)
    #
    #         train_loss_bag = train(train_index, milnet, criterion, optimizer, args)  # iterate all bags
    #         test_loss_bag, avg_score, aucs, thresholds_optimal = test(test_index, milnet, criterion, optimizer, args)
    #         print('\r Epoch [%d/%d] train loss: %.4f test loss: %.4f, average score: %.4f, AUC: ' %
    #               (epoch, args.num_epochs, train_loss_bag, test_loss_bag, avg_score) + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #         scheduler.step()
    #         current_score = (sum(aucs) + avg_score) / 2  # 均衡考虑auc与accuracy
    #
    #         if current_score >= best_score:
    #             best_score = current_score
    #             print('Best_score thresholds ===>>> ' + '|'.join(
    #                 'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #
    #         if aucs[1] >= best_auc:
    #             best_auc = aucs[1]
    #             print('Best_auc thresholds ===>>> ' + '|'.join(
    #                 'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #             print(f'Best auc ===>>>{best_auc}')
    #
    #         if sum(aucs) >= best_avg_auc:
    #             best_avg_auc = sum(aucs)
    #             print('Best_avg_auc thresholds ===>>> ' + '|'.join(
    #                 'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #             print('best avg_auc ===>>>' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #         if best_loss == 0:
    #             best_loss = test_loss_bag
    #             if test_loss_bag < best_loss:
    #                 best_loss = test_loss_bag
    #                 best_loss_score = current_score
    #                 best_loss_auc = aucs[1]
    #                 best_loss_avg_auc = sum(aucs)
    #                 best_loss_epoch = epoch
    #     with open(f'results_{args.run}.txt', 'a+') as f:
    #         f.write(f'{args.lr},{args.weight_decay},{args.Tmax},{best_score},{best_auc},{best_avg_auc},\
    #         {best_loss_epoch},{best_loss_score},{best_loss_auc},{best_loss_avg_auc}\n')
    #
    # else:
    #     writer = SummaryWriter(f'run/lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}') # path saving tensorboard logdir
    #     save_path = os.path.join('weights', f'lrwdT_{args.lr}_{args.weight_decay}_{args.Tmax}')  # path saving models
    #     os.makedirs(save_path, exist_ok=True)
    #
    #     if args.record == 'best':
    #         for epoch in range(1, args.num_epochs):
    #             train_index = shuffle(train_index)
    #             test_index = shuffle(test_index)
    #
    #             train_loss_bag = train(train_index, milnet, criterion, optimizer, args)  # iterate all bags
    #             test_loss_bag, avg_score, aucs, thresholds_optimal = test(test_index, milnet, criterion, optimizer, args)
    #             writer.add_scalar('train_loss', train_loss_bag, epoch)
    #             writer.add_scalar('val_loss', test_loss_bag, epoch)
    #             writer.add_scalar('avg_score', avg_score, epoch)
    #             writer.add_scalar('class 0 aucs', aucs[0], epoch)
    #             writer.add_scalar('class_1 aucs', aucs[1], epoch)
    #             print('\r Epoch [%d/%d] train loss: %.4f test loss: %.4f, average score: %.4f, AUC: ' %
    #                   (epoch, args.num_epochs, train_loss_bag, test_loss_bag, avg_score) + '|'.join(
    #                 'class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #             scheduler.step()
    #             current_score = (sum(aucs) + avg_score) / 2  # 均衡考虑auc与accuracy
    #
    #             if current_score >= best_score:
    #                 best_score = current_score
    #                 save_name = os.path.join(save_path, 'best_score.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_score thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #
    #             if aucs[1] >= best_auc:
    #                 best_auc = aucs[1]
    #                 save_name = os.path.join(save_path, 'best_auc.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_auc thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #                 print(f'\nBest auc ===>>>{best_auc}')
    #
    #             if sum(aucs) >= best_avg_auc:
    #                 best_avg_auc = sum(aucs)
    #                 save_name = os.path.join(save_path, 'best_avg_auc.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_avg_auc thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #                 print('\n best avg_auc ===>>>' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #             if best_loss == 0:
    #                 best_loss = test_loss_bag
    #                 if test_loss_bag < best_loss:
    #                     best_loss = test_loss_bag
    #                     best_loss_score = current_score
    #                     best_loss_auc = aucs[1]
    #                     best_loss_avg_auc = sum(aucs)
    #                     best_loss_epoch = epoch
    #                     save_name = os.path.join(save_path, 'best_loss.pth')
    #                     torch.save(milnet.state_dict(), save_name)
    #         with open(f'results_{args.run}.txt', 'a+') as f:
    #             f.write(f'{args.lr},{args.weight_decay},{args.Tmax},{best_score},{best_auc},{best_avg_auc},\
    #                         {best_loss_epoch},{best_loss_score},{best_loss_auc},{best_loss_avg_auc}\n')
    #     elif args.record == 'all':
    #         for epoch in range(1, args.num_epochs):
    #             train_index = shuffle(train_index)
    #             test_index = shuffle(test_index)
    #
    #             train_loss_bag = train(train_index, milnet, criterion, optimizer, args)  # iterate all bags
    #             test_loss_bag, avg_score, aucs, thresholds_optimal = test(test_index, milnet, criterion, optimizer,
    #                                                                       args)
    #             writer.add_scalar('train_loss',train_loss_bag,epoch)
    #             writer.add_scalar('avg_score',avg_score,epoch)
    #             writer.add_scalar('class 0 aucs',aucs[0],epoch)
    #             writer.add_scalar('class_1 aucs',aucs[1],epoch)
    #             print('\r Epoch [%d/%d] train loss: %.4f test loss: %.4f, average score: %.4f, AUC: ' %
    #                   (epoch, args.num_epochs, train_loss_bag, test_loss_bag, avg_score) + '|'.join(
    #                 'class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #             scheduler.step()
    #             current_score = (sum(aucs) + avg_score) / 2  # 均衡考虑auc与accuracy
    #
    #             if current_score >= best_score:
    #                 best_score = current_score
    #                 save_name = os.path.join(save_path, str(epoch)+'best_score.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_score thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #
    #             if aucs[1] >= best_auc:
    #                 best_auc = aucs[1]
    #                 save_name = os.path.join(save_path, str(epoch)+'best_auc.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_auc thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #                 print(f'Best auc ===>>>{best_auc}')
    #
    #             if sum(aucs) >= best_avg_auc:
    #                 best_avg_auc = sum(aucs)
    #                 save_name = os.path.join(save_path, str(epoch)+'best_avg_auc.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #
    #                 print('Best model saved at: ' + save_name)
    #                 print('Best_avg_auc thresholds ===>>> ' + '|'.join(
    #                     'class-{}>>{}'.format(*k) for k in enumerate(thresholds_optimal)))
    #                 print('best avg_auc ===>>>' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs)))
    #             if best_loss == 0:
    #                 best_loss = test_loss_bag
    #             elif test_loss_bag < best_loss:
    #                 best_loss = test_loss_bag
    #                 best_loss_score = current_score
    #                 best_loss_auc = aucs[1]
    #                 best_loss_avg_auc = sum(aucs)
    #                 best_loss_epoch = epoch
    #                 save_name = os.path.join(save_path, str(epoch) + 'best_loss.pth')
    #                 torch.save(milnet.state_dict(), save_name)
    #         with open(f'results_{args.run}.txt', 'a+') as f:
    #             f.write(f'{args.lr},{args.weight_decay},{args.Tmax},{best_score},{best_auc},{best_avg_auc},\
    #                         {best_loss_epoch},{best_loss_score},{best_loss_auc},{best_loss_avg_auc}\n')

if __name__ == '__main__':
    tick = time.time()
    main()
    print(time.time() - tick)

# saved form:(args.lr,args.weight_decay,args.Tmax,best_score,best_auc,best_avg_auc,\
# best_loss_epoch,best_loss_score,best_loss_auc,best_loss_avg_auc)
