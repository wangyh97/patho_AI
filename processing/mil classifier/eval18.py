import logging
import random
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.autograd import Variable
import torchvision.transforms.functional as VF
from torchvision import transforms

import sys, argparse, os, copy, itertools, glob, datetime
import pandas as pd
import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support
from sklearn.datasets import load_svmlight_file
from collections import OrderedDict
import time

'''
similar to train_tcga, use testing set to eval
changes:
    test_index: grouping['val_list'].index --> grouping['test_list'].index
    del training_index
    del saving_path
    del train()
    model loading path:../init.pth --> pth file path
    change to eval mode:
        model.eval()
        with torch.no_grad()
    del arg: epoch;run
    result saving file: result_{run}.txt  -->  test_result_{run}.txt
'''

# load data

# =====>>>> current working dir is where the bash file located, not the executed py file <<<<==============

features = np.load('../../../data/pretrained_resnet18/10X_full_slide_features_PtRes18.npy', allow_pickle=True).item()
# train = np.load('../../config/data_segmentation_csv/10X_tv_grouping.npy',allow_pickle=True).item()
full = np.load('../../../config/data_segmentation_csv/10X_full.npy', allow_pickle=True).item()
# train_index = list(train['tv_list'].index)
full_index = list(full['full_list'].index)
# test_index = list(set(full_index)-set(train_index))
grouping = np.load('../../../config/data_segmentation_csv/10X_grouping.npy', allow_pickle=True).item()

test_index = grouping['test_list'].index

index_dict = {ind: i for i, ind in enumerate(
    full_index)}  # feature dict -- index{i} is generated by iloc, meaning i is continuous index except the original uncontinuous ones, need transform using index read from grouping infos
label_dict = {'L': 0, 'H': 1}


def timer(func):
    def wrapper(*args, **kws):
        tick = time.time()
        func(*args, **kws)
        print(f'{func.__name__} comsumes {time.time() - tick} s')

    return wrapper


def set_seed(seed=10):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    print('seed set')


#     torch.backends.cudnn.benchmark = False
#     torch.backends.cudnn.deterministic = True


def get_bag_feats(index, args):
    """
    example of label/feature : get_bag_feats(test_index[2],args)
    label:[1. 0.]
    feature:[[2.5330880e+00 1.6554745e-01 8.0470458e-02 ... 1.3655423e+00
      5.7236932e-02 5.2817654e-02]
     [2.8611205e+00 2.9038048e-01 6.0187571e-02 ... 1.8358935e+00
      5.3482568e-01 6.2871813e-03]
     [3.3081994e+00 7.3715396e-02 1.2616467e+00 ... 1.9259404e+00
      1.4681002e-01 2.8594225e-03]
     ...
     [2.9693909e+00 3.2868910e-01 1.3055435e-01 ... 2.4260533e+00
      1.7651926e-01 1.2930447e-01]
     [2.8800142e+00 3.0109720e-02 8.2876140e-01 ... 2.4528553e+00
      5.6700967e-03 0.0000000e+00]
     [1.4685658e+00 1.6393182e-01 6.0487707e-04 ... 1.2453270e+00
      0.0000000e+00 4.1464632e-03]]
    """
    global features
    global index_dict

    feats_og = pd.DataFrame(features[f'index{index_dict[index]}'][2])
    feats = shuffle(feats_og).reset_index(drop=True).to_numpy()
    label_og = label_dict[features[f'index{index_dict[index]}'][1]]  # transformed label in form of int,[0,1]

    label = np.zeros(args.num_classes)
    if args.num_classes == 1:
        label[0] = label_og
    else:
        if int(label_og) <= (len(label) - 1):
            label[int(label_og)] = 1
    return label, feats


def dropout_patches(feats, p):
    idx = np.random.choice(np.arange(feats.shape[0]), int(feats.shape[0] * (1 - p)), replace=False)
    sampled_feats = np.take(feats, idx, axis=0)
    pad_idx = np.random.choice(np.arange(sampled_feats.shape[0]), int(feats.shape[0] * p), replace=False)
    pad_feats = np.take(sampled_feats, pad_idx, axis=0)
    sampled_feats = np.concatenate((sampled_feats, pad_feats), axis=0)
    return sampled_feats


def test(test_df, milnet, criterion, args):
    """
    demo of values of intermediate variables:

    original label:tensor([[0., 1.]], device='cuda:0'),shape of feats:torch.Size([1, 53, 512])
    shape of feats after view:torch.Size([53, 512])
    shape of ins_pred:torch.Size([53, 2]),original bag_pred:tensor([[-2.9899,  2.7934],
            [-3.7077,  3.5751],
            [-3.6781,  3.7495],
            [-3.8786,  3.5615]], device='cuda:0'),shape of original bag_pred:torch.Size([4, 2])
    max pred:tensor([ 0.9809, -2.4702], device='cuda:0'),bag pred after mean:tensor([-3.5636,  3.4199], device='cuda:0')
     Testing bag [0/73] bag loss: 0.9777
    test laels:[0. 1.],test prediction : [array([0.02755601, 0.9683193 ], dtype=float32)]
    first 5 class_pred_bag:[1. 0. 0. 0. 0.],
    first 5 test_pred:[[0. 1.]
     [0. 0.]
     [1. 0.]
     [1. 0.]
     [1. 0.]],
     first 5 labels:[[0. 1.]
     [1. 0.]
     [1. 0.]
     [1. 0.]
     [1. 0.]]
     first 5 single char labels:[1, 0, 0, 0, 0],first 5 single char pred:[1, 0, 0, 1, 0]
     """
    milnet.eval()
    total_loss = 0
    test_labels = []
    test_predictions = []
    Tensor = torch.cuda.FloatTensor
    #     Tensor = torch.FloatTensor
    with torch.no_grad():
        for i in range(len(test_df)):
            label, feats = get_bag_feats(test_df[i], args)
            bag_label = Variable(Tensor([label]))
            bag_feats = Variable(Tensor([feats]))
            bag_feats = bag_feats.view(-1, args.feats_size)
            ins_prediction, bag_prediction, _, _ = milnet(bag_feats)
            max_prediction, _ = torch.max(ins_prediction, 0)
            bag_prediction = torch.mean(bag_prediction, dim=0)
            bag_loss = criterion(bag_prediction.view(1, -1), bag_label.view(1, -1))
            max_loss = criterion(max_prediction.view(1, -1), bag_label.view(1, -1))
            loss = 0.5 * bag_loss + 0.5 * max_loss
            total_loss = total_loss + loss.item()
            sys.stdout.write('\r Testing bag [%d/%d] bag loss: %.4f' % (i, len(test_df), loss.item()))
            test_labels.extend([label])
            if args.average:
                test_predictions.extend([(0.5 * torch.sigmoid(max_prediction) + 0.5 * torch.sigmoid(
                    bag_prediction)).squeeze().cpu().numpy()])
            else:
                test_predictions.extend([(0.0 * torch.sigmoid(max_prediction) + 1.0 * torch.sigmoid(
                    bag_prediction)).squeeze().cpu().numpy()])
    test_labels = np.array(test_labels)
    test_predictions = np.array(test_predictions)
    auc_value, _, thresholds_optimal = multi_label_roc(test_labels, test_predictions, args.num_classes, pos_label=1)
    if args.num_classes == 1:
        class_prediction_bag = copy.deepcopy(test_predictions)
        class_prediction_bag[test_predictions >= thresholds_optimal[0]] = 1
        class_prediction_bag[test_predictions < thresholds_optimal[0]] = 0
        test_predictions = class_prediction_bag
        test_labels = np.squeeze(test_labels)
    else:
        for i in range(args.num_classes):
            class_prediction_bag = copy.deepcopy(test_predictions[:, i])
            class_prediction_bag[test_predictions[:, i] >= thresholds_optimal[i]] = 1
            class_prediction_bag[test_predictions[:, i] < thresholds_optimal[i]] = 0
            test_predictions[:, i] = class_prediction_bag
    # get confusion matrix
    one_hot_labels = [np.argmax(i) for i in test_labels]
    one_hot_preds = [np.argmax(i) for i in test_predictions]
    c = confusion_matrix(one_hot_labels, one_hot_preds).ravel()
    # tn, fp, fn, tp = c.ravel()
    # get acc
    bag_score = 0
    for i in range(0, len(test_df)):
        bag_score = np.array_equal(test_labels[i], test_predictions[i]) + bag_score
    avg_score = bag_score / len(test_df)
    return avg_score, auc_value, c


def multi_label_roc(labels, predictions, num_classes, pos_label=1):
    fprs = []
    tprs = []
    thresholds = []
    thresholds_optimal = []
    aucs = []
    if len(predictions.shape) == 1:
        predictions = predictions[:, None]
    for c in range(0, num_classes):
        label = labels[:, c]
        prediction = predictions[:, c]
        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=1)
        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)
        c_auc = roc_auc_score(label, prediction)
        aucs.append(c_auc)
        thresholds.append(threshold)
        thresholds_optimal.append(threshold_optimal)
    return aucs, thresholds, thresholds_optimal


def optimal_thresh(fpr, tpr, thresholds, p=0):
    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)
    idx = np.argmin(loss, axis=0)
    return fpr[idx], tpr[idx], thresholds[idx]


@timer
def main():
    parser = argparse.ArgumentParser(description='Train DSMIL on 20x patch features learned by SimCLR')
    # fixed args
    parser.add_argument('--num_classes', default=2, type=int, help='Number of output classes [2]')
    parser.add_argument('--feats_size', default=512, type=int, help='Dimension of the feature size [512]')
    parser.add_argument('--gpu_index', type=str, help='GPU ID(s) [0]')
    parser.add_argument('--model', default='dsmil', type=str, help='MIL model [dsmil]')
    parser.add_argument('--dropout_patch', default=0, type=float, help='Patch dropout rate [0]')
    parser.add_argument('--dropout_node', default=0, type=float, help='Bag classifier dropout rate [0]')
    parser.add_argument('--non_linearity', default=1, type=float, help='Additional nonlinear operation [0]')
    parser.add_argument('--average', type=bool, default=True,
                        help='Average the score of max-pooling and bag aggregating')
    # custom args
    parser.add_argument('--run', type=str,
                        help='run number for documentation, saving results in file: test_results{run}.txt')
    parser.add_argument('--path', type=str, help='path of pth files')
    args = parser.parse_args()
    #     gpu_ids = args.gpu_index
    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_index)
    set_seed()
    # initiate logger: show info in stream & save in file
    log = logging.getLogger('recorder')
    handler = logging.FileHandler('metrics.log')
    handler1 = logging.StreamHandler
    handler.setLevel(logging.DEBUG)
    handler1.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')
    formatter1 = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    handler1.setFormatter(formatter1)
    log.addHandler(handler)
    log.addHandler(handler1)

    if args.model == 'dsmil':
        import dsmil as mil
    elif args.model == 'abmil':
        import abmil as mil

    i_classifier = mil.FCLayer(in_size=args.feats_size, out_size=args.num_classes).cuda()
    b_classifier = mil.BClassifier(input_size=args.feats_size, output_class=args.num_classes,
                                   dropout_v=args.dropout_node, nonlinear=args.non_linearity).cuda()
    milnet = mil.MILNet(i_classifier, b_classifier).cuda()

    milnet = torch.nn.DataParallel(milnet)
    milnet = milnet.cuda()

    if args.model == 'dsmil':
        state_dict_weights = torch.load(args.path)
        try:
            milnet.load_state_dict(state_dict_weights, strict=False)
        except:
            del state_dict_weights['b_classifier.v.1.weight']
            del state_dict_weights['b_classifier.v.1.bias']
            milnet.load_state_dict(state_dict_weights, strict=False)
    criterion = nn.BCEWithLogitsLoss()

    # load data
    global test_index

    milnet.eval()
    with torch.no_grad():
        avg_score, aucs, conf_mat = test(test_index, milnet, criterion, args)
        log.info('record of model performance, based on PtRes18 features & tuned hyperparamter using dsmil')
        log.info(f'loaded module:{args.path}')
        log.info(f'avg_score: {avg_score}')
        log.info(f'auc for TMB-H: {aucs[1]}')
        log.info(f'tn, fp, fn, tp:{conf_mat.ravel()[0],conf_mat.ravel()[1],conf_mat.ravel()[2],conf_mat.ravel()[3]}')

if __name__ == '__main__':
    main()
