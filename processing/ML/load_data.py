import random
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.autograd import Variable
import torchvision.transforms.functional as VF
from torchvision import transforms

import sys, argparse, os, copy, itertools, glob, datetime
import pandas as pd
import numpy as np
import h5py
from sklearn.utils import shuffle
from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support
from sklearn.model_selection import StratifiedKFold
from sklearn.datasets import load_svmlight_file
from collections import OrderedDict
import time
import timm
import timm.optim
import timm.scheduler

os.chdir('/GPUFS/sysu_jhluo_1/wangyh/project/BLCA_TMB/processing')

# read features
## PtRes style，read all features
def load_data_DictforDsmil(scale,description,augmented=False):
    features = np.load(f'../data/{description}/{scale}X_full_slide_features.npy',
                       allow_pickle=True).item()
    if augmented:
        augmented_features = 0
        # todo
    return features

## CLAM style, read features sample by sample
def load_data_h5(scale,description,sample_id,augmented=False):  # from CLAM style h5 file
    data_path = f'CLAM_master/{scale}X_{description}/h5_files/{sample_id}.h5'
    features_container = []
    coords_container = []
    with h5py.File(data_path, 'r') as f:
        features_container.append(f['features'][:])
        coords_container.append(['coords'][:])
    if augmented:
        augmented_features = 0
        # todo
    features = features_container[0]
    coords = coords_container[0]
    return features,coords


# read splits
## PtRes style，read config/split file ,returns train index list (keys when parsing features,labels,uuids in full_slide_features data file)
def read_k_splits_Dsmil(scale,K):
    train = np.load(f'../config/data_segmentation_csv/{scale}X_tv_grouping.npy', allow_pickle=True).item()  # training set
    full = np.load(f'../config/data_segmentation_csv/{scale}X_full.npy', allow_pickle=True).item()
    full_index = list(full['full_list'].index)
    train_index = list(train['tv_list'].index)  # 用于k折 train/val的数据的原始index
    test_index = list(set(full_index)-set(train_index))  # 用于独立验证的test的数据的原始index

    train_list = []
    val_list = []
    if K != 0:
        X = train_index
        y = [train['tv_list'][train['tv_list']['dir_uuid'] == uuid] for uuid in X]
        sKF = StratifiedKFold(n_splits=K)
        for train_iloc, val_iloc in sKF.split(X, y):
            train_list.append([train_index[i] for i in train_iloc])
            val_list.append([train_index[i] for i in val_iloc])
    else:
        # 这里应该加载  config/data_segmentation_csv/{scale}X_grouping.npy 文件，获取train / val的分组
        pass

    index_dict = {ind: i for i, ind in enumerate(
        full_index)}  # feature dict -- index{i} is generated by iloc, meaning i is continuous index except the original uncontinuous ones, need transform using index read from grouping infos
    return train_list, val_list, test_index, index_dict

## CLAM style, read CLAM_mster/splits, split file need to be generated first using CLAM_master/create_split_fp.py
def read_k_splits_h5(scale,K):
    split_path = f'CLAM_master/splits/{scale}X_pretrained_resnet0_64/splits_{K}.csv'
    split_df = pd.read_csv(split_path)
    train_list = split_df['train'].values.tolist()
    val_list = split_df['val'].values.tolist()
    test_list = split_df['test'].values.tolist()
    return train_list, val_list, test_list


# get single bag feature with labels
## PtRes style
def get_bag_feats_DsmilDict(features,index,index_dict,args):
    label_dict = {'L': 0, 'H': 1}
    try:
        feats_og = pd.DataFrame(features[f'index{index_dict[index]}'][2])
        feats = shuffle(feats_og).reset_index(drop=True).to_numpy()  # feats are shuffled before entering
        label_og = label_dict[features[f'index{index_dict[index]}'][1]]  # transformed label in form of int,[0,1]

        label = np.zeros(args.num_classes)
        if args.num_classes == 1:
            label[0] = label_og
        else:
            if int(label_og) <= (len(label) - 1):
                label[int(label_og)] = 1
        return label, feats
    except:
        print('invalid data, may be removed during reduce')

## CLAM style
def get_bag_feats_h5(scale,description,sample_id,augmented=False):
    feature, coords = load_data_h5(scale,description,sample_id,augmented)
    df = pd.read_csv('../config/TCGA_TMB.csv')
    continuous_label = df.loc[df['Tumor_Sample_ID'] == sample_id, 'TMB'].values[0]
    binary_label = continuous_label >= 10  # binary label is assigned 1/0 by condition if TMB >= 10, returns a single number
    return feature,coords,continuous_label,binary_label


#  get instance feature with pseudolabels, return dataframe for machine learning
## single sample: instance level with pseudolabels
def _get_instance_feats_with_pseudolabels(scale,description,sample_id):
    path_with_pseudolabels = ''
    data = {}
    with h5py.File(path_with_pseudolabels, 'r') as f:
        data['instances'] = f['features'][:]
        data['continuous_pseudolabels'] = f['continuous_pseudolabels'][:]
        data['binary_pseudolabels'] = f['binary_pseudolabels'][:]
    df = pd.DataFrame(data['instances'])
    df['continuous_pseudolabels'] = data['continuous_pseudolabels']
    df['binary_pseudolabels'] = data['binary_pseudolabels']
    return df

## all samples: instance level with pseudolabels
def get_all_instance_feats_with_pseudolabels(scale,description): # todo
    path_with_pseudolabels = ''
    data = {}
    with h5py.File(path_with_pseudolabels, 'r') as f:
        data['instances'] = f['features'][:]
        data['continuous_pseudolabels'] = f['continuous_pseudolabels'][:]
        data['binary_pseudolabels'] = f['binary_pseudolabels'][:]
    df = pd.DataFrame(data['instances'])
    df['continuous_pseudolabels'] = data['continuous_pseudolabels']
    df['binary_pseudolabels'] = data['binary_pseudolabels']
    return df

## generate bag level dataframe with bag_feats & labels
def get_bag_feats_with_pseudolabels(scale,description,sample_id):
    path_with_pseudolabels = ''
    data = {}
    with h5py.File(path_with_pseudolabels, 'r') as f:
        data['instances'] = f['features'][:]
        data['continuous_pseudolabels'] = f['continuous_pseudolabels'][:]
        data['binary_pseudolabels'] = f['binary_pseudolabels'][:]
    df = pd.DataFrame(data['instances'])
    df['continuous_pseudolabels'] = data['continuous_pseudolabels']
    df['binary_pseudolabels'] = data['binary_pseudolabels']
    return df

# generate test code in main
if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument('--num_classes', type=int, default=2)
    args = args.parse_args()

    features = load_data_DictforDsmil('64','pretrained_resnet0_64')
    train_list, val_list, test_index, index_dict = read_k_splits_Dsmil('64',5)
    print(train_list)
    print(val_list)
    print(test_index)
    print(index_dict)
    print(get_bag_feats_DsmilDict(features,test_index[0],index_dict,args))
    print(get_bag_feats_h5('64','pretrained_resnet0_64','TCGA-A2-A0TU'))