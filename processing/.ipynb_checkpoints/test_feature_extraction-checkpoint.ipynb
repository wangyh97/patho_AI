{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a812c032-312b-40d5-b3ce-10bba8de10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyh/miniconda3/envs/patho_AI/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models as torch_models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad52ca-fe61-4294-9b1b-c47e68787f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='The start and end positions in the file list')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='GPU device')\n",
    "    parser.add_argument('--start', type=float, default=0.0, help='start position')\n",
    "    parser.add_argument('--end', type=float, default=0.01, help='end position')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = 1\n",
    "\n",
    "    dataset = 'TCGA-BRCA'\n",
    "    data_dir = '../data/{:s}'.format(dataset)\n",
    "    all_patch_indices = np.load('{:s}/20x_512x512/clustering_results/all_patch_indices_refined.npy'.format(data_dir),\n",
    "                                allow_pickle=True).item()\n",
    "\n",
    "    img_dir = '{:s}/20x_normalized'.format(data_dir)\n",
    "    save_dir = '{:s}/20x_features_resnet101'.format(data_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    data_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    batch_size = 64\n",
    "    model = ResNet_extractor(layers=101).cuda()\n",
    "    model = model.eval()\n",
    "\n",
    "    slides_list = pd.read_csv('{:s}/slide_selection_final.txt'.format(data_dir), header=None)\n",
    "    slides_list = list(slides_list[0].values)\n",
    "    N = len(slides_list)\n",
    "    slides_to_be_processed = slides_list[int(N * args.start):int(N * args.end)]\n",
    "\n",
    "    count = 0\n",
    "    for slide_name in tqdm(slides_to_be_processed):\n",
    "        indices = all_patch_indices[slide_name]\n",
    "        count += 1\n",
    "        # if count < 49:\n",
    "        #     continue\n",
    "\n",
    "        tumor_indices = indices['tumor']\n",
    "        N_tumor_patch = len(tumor_indices)\n",
    "        feature_list = []\n",
    "        index_list = []\n",
    "        for batch_idx in range(0, N_tumor_patch, batch_size):\n",
    "            end = batch_idx + batch_size if batch_idx+batch_size < N_tumor_patch else N_tumor_patch\n",
    "            indices = tumor_indices[batch_idx: end]\n",
    "            images = []\n",
    "            for idx in indices:\n",
    "                image = Image.open('{:s}/{:s}/{:d}.png'.format(img_dir, slide_name, int(idx))).convert('RGB')\n",
    "                image_tensor = data_transform(image).unsqueeze(0)\n",
    "                images.append(image_tensor)\n",
    "            images = torch.cat(images, dim=0)\n",
    "\n",
    "            features = model(images.cuda())\n",
    "            feature_list.append(features.detach().cpu())\n",
    "            index_list += list(indices)\n",
    "            del features\n",
    "\n",
    "        feature_list = torch.cat(feature_list, dim=0)\n",
    "        np.save('{:s}/{:s}.npy'.format(save_dir, slide_name), {'features': feature_list.numpy(), 'indices': np.array(index_list)})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a017673c-3490-4558-92fa-ce407d4a38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = np.load('../config/data_segmentation_csv/10X_grouping.npy',allow_pickle=True).item()\n",
    "test_df = data_df['test_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29286470-cee7-49c9-afa0-b9b163981b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cb51f6-9076-43f4-9ade-eb444a8ad5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = '../data'\n",
    "paths = os.path.join(save_dir,'slide_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd1c97f-6965-4402-80fd-306af29831fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/slide_features.npy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedc948f-29fe-42c0-a41f-1fc1d41470d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimclr_feature_extractor/runs/Apr05_00-53-45_ubuntu-T640/checkpoints/model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cbbe2e8-b1f0-4bff-88d0-811bcfd6d858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome/wangyh/uro_biomarker/patho_AI/processing/simclr_feature_extractor/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet_simclr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNetSimCLR\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('home/wangyh/uro_biomarker/patho_AI/processing/simclr_feature_extractor/')\n",
    "from resnet_simclr import ResNetSimCLR\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3987fbde-386f-4f5a-bf03-2eadd0ac23f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1590173131.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    model =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('simclr_feature_extractor/runs/Apr05_00-53-45_ubuntu-T640/checkpoints/model.pth')\n",
    "model = \n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a1efd-a47c-49f2-aae1-50d2ff5f0a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afc50e-b55b-46f0-b2cf-6404b8235792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patho_AI",
   "language": "python",
   "name": "patho_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
